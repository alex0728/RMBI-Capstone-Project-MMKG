# RMBI-Capstone-Project-MMKG

The purpose of this project is to constitute a prototype of a Multimodal knowledge graph
(MMKG) which aims to achieve higher efficiency and accuracy.

In the past few years, Hyper-personalization (HP) has become a crucial concept for both
E-commerce and traditional business alike. HP is done by creating customized and targeted
experiences through the use of data, analytics, AI, and automation. Knowledge Graph (KG) is
one of the backbone technologies applied in the foundation level of HP applications. It
provides ways to efficiently organize, manage and retrieve information, and has been
increasingly used as an external source of knowledge for HP applications like recommender
systems, language modeling, question answering or image classification. This technology
demonstrates promising results by encoding entity and relation information of textual
knowledge in the graph.

However, most KG construction work in the industry still focuses on organizing and
analysing only textual knowledge in a structured representation. There is a relatively small
amount of attention in capitalizing visual resources for KG research (Wang, Wang, Qi, &
Zheng, 2020). A visual database is normally an affluent source of image or video data and
provides adequate visual information and relational hints about entities in KGs. Making link
prediction and entity alignment in a wider scope can empower models to boost on
performance when considering a combination of textual and visual features.

In light of the aforementioned approach, a Multimodal Knowledge Graph (MMKG)
containing image, text, video, and knowledge (unstructured, semi-structured and structured)
would be a solution to achieve higher efficiency and accuracy in HP tasks, and would better
suit the need for E-commerce and traditional business in the 21st century.
